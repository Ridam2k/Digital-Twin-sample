# Digital Twin - RAG-based Dual Persona AI Assistant

A sophisticated Retrieval-Augmented Generation (RAG) system that creates a personalized AI assistant with dual personas (technical and non-technical). The system ingests data from Google Drive, GitHub repositories, and synthetic sources, then provides intelligent responses with proper source attribution.

## ğŸŒŸ High-Level User Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA INGESTION                            â”‚
â”‚  Google Drive (Docs/PDFs/Slides) + GitHub Repos + Synthetic     â”‚
â”‚                            â†“                                     â”‚
â”‚                  Chunking & Embedding                            â”‚
â”‚                            â†“                                     â”‚
â”‚                    Qdrant Vector DB                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER QUERY                                â”‚
â”‚                            â†“                                     â”‚
â”‚              Mode Detection (Technical/Non-technical)            â”‚
â”‚                            â†“                                     â”‚
â”‚        Semantic Retrieval (with optional @code filtering)        â”‚
â”‚                            â†“                                     â”‚
â”‚              Context Building + LLM Generation                   â”‚
â”‚                            â†“                                     â”‚
â”‚              Response with Citations & Evaluation                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Flow:
1. **Ingestion**: Documents are fetched from Google Drive folders, GitHub repositories, and local synthetic data
2. **Processing**: Content is chunked, tagged with metadata (personality namespace, content type), and embedded
3. **Storage**: Embeddings stored in Qdrant vector database with metadata for filtering
4. **Query Processing**:
   - User submits query (optionally prefixed with `@code` for code-specific queries)
   - Router detects appropriate personality mode (technical/non-technical)
   - Retriever performs semantic search with optional content-type filtering
   - Context builder assembles relevant chunks with system prompts
   - Generator produces response with proper citations
5. **Evaluation**: Optional groundedness and persona consistency checks
6. **Frontend**: React-based UI with conversation interface and observability metrics

---

## ğŸ“‹ Prerequisites

- **Python**: 3.9+ (developed with Python 3.12)
- **Node.js**: 16+ (for frontend)
- **pip** or **uv** package manager
- **Git** (for cloning repositories)

---

## ğŸš€ Setup Instructions

### 1. Clone the Repository

```bash
git clone <repository-url>
cd DT
```

### 2. Backend Setup

#### Create Virtual Environment

```bash
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

#### Install Dependencies

```bash
uv sync
```

### 3. Configuration Files (Provided Offline)

The following files contain sensitive credentials and will be provided separately:

- **`credentials.json`**: Google Drive OAuth credentials
- **`token.json`**: Google Drive authentication token
- **`config.py`**: Contains API keys and configuration:
  - Qdrant database URL and API key
  - OpenAI API keys (for embeddings and generation)
  - Google Drive folder IDs (technical and non-technical) 
  - GitHub personal access token
  - Collection names, chunk sizes, and other parameters

- The project setup currently connects to my(Ridam Srivastava's) Google Drive folders and Github Repos that have been granted access for the same; configuring to your own Drive and Github would require additional steps for setting up access

**âš ï¸ Important**: Place these files in the project root directory before proceeding

### 4. Frontend Setup

```bash
cd frontend
npm install
cd ..
```

---

## ğŸƒ Running the Application

### Option 1: Full Stack (Recommended)

#### Terminal 1 - Start Backend API Server

```bash
source .venv/bin/activate  # Activate virtual environment
uvicorn api_server:app --reload --port 8000
```

The API will be available at `http://localhost:8000`

#### Terminal 2 - Start Frontend Development Server

```bash
cd frontend
npm run dev
```

The UI will be available at `http://localhost:3000`

### Option 2: CLI Query Interface

For quick testing without the UI:

```bash
source .venv/bin/activate
python query_cli.py
```

**Usage**:
- Type queries directly
- Prefix with `@code` for code-specific queries (e.g., `@code how does authentication work?`)
- Type `exit` to quit

---

## ğŸ“Š Data Ingestion

### Initial Data Ingestion

- The following files would be provided to DIRECTLY run the retrieval: data/gdrive_hash_store.json, data/github_hash_store.json, data/synthetic_hash_store.json
- If these files are put into the data folder, then the ingestion step DOES NOT need to be run for the app to be run locally, as the associated files are ALREADY present in the QDRANT client

- If running the ingestion flow from scratch:
- First run the deletions script to delete the existing data points: 

Then ingest data sources as:

```bash
source .venv/bin/activate
python main_ingest.py
```

This will:
- Fetch documents from configured Google Drive folders (technical & non-technical)
- Clone and process configured GitHub repositories
- Process synthetic JSON documents from `data/sources/`
- Embed and store everything in Qdrant

**Note**: The system uses hash-based change detection, so subsequent runs only process new or modified files.

### Ingestion Details

- **Google Drive**: Supports Google Docs, PDFs, and Presentations
- **GitHub**: Processes code files (`.py`, `.js`, `.jsx`, `.css`, `.html`, `.ipynb`) and documentation (`.md`)
- **Synthetic**: Custom JSON documents from `data/sources/` directory

---

## ğŸ—ï¸ Project Structure

```
DT/
â”œâ”€â”€ api_server.py          # FastAPI backend server
â”œâ”€â”€ main_ingest.py         # Data ingestion pipeline
â”œâ”€â”€ query_cli.py           # CLI interface
â”œâ”€â”€ config.py              # Configuration (gitignored)
â”œâ”€â”€ credentials.json       # Google OAuth (gitignored)
â”œâ”€â”€ token.json             # Google token (gitignored)
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ pyproject.toml         # Python project metadata
â”œâ”€â”€ uv.lock                # uv lockfile
â”œâ”€â”€ generate_eval.py       # Evaluation dataset generation
â”œâ”€â”€ generate_synthetic_data.py # Synthetic data generator
â”œâ”€â”€ eval_retrieval.py      # Retrieval evaluation runner
â”œâ”€â”€ eval_set.json          # Evaluation dataset
â”œâ”€â”€ eval_log.jsonl         # Evaluation run logs
â”œâ”€â”€ retrieval_stats.json   # Aggregated retrieval stats
â”œâ”€â”€ quick_fix.py           # Utility script
â”œâ”€â”€ test_persona_consistency.py # Tests
â”œâ”€â”€ test_bleed_full.py     # Tests
â”œâ”€â”€ verify_checkpoint1.py  # Checkpoint verification
â”œâ”€â”€ verify_checkpoint2.py  # Checkpoint verification
â”œâ”€â”€ verify_checkpoint3.py  # Checkpoint verification
â”‚
â”œâ”€â”€ core/                  # Core RAG pipeline modules
â”‚   â”œâ”€â”€ router.py          # Mode detection (technical/non-technical)
â”‚   â”œâ”€â”€ retriever.py       # Semantic search
â”‚   â”œâ”€â”€ context_builder.py # Prompt assembly
â”‚   â”œâ”€â”€ generator.py       # LLM response generation
â”‚   â”œâ”€â”€ groundedness.py    # Response grounding evaluation
â”‚   â”œâ”€â”€ persona_consistency.py  # Persona alignment checks
â”‚   â”œâ”€â”€ retrieval_metrics.py    # Retrieval quality metrics
â”‚   â”œâ”€â”€ eval_aggregator.py  # Eval aggregation utilities
â”‚   â”œâ”€â”€ pdf_extractor.py    # PDF parsing utilities
â”‚   â”œâ”€â”€ identity.py         # Persona/identity helpers
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ingest/                # Ingestion modules
â”‚   â”œâ”€â”€ gdrive_reader.py   # Google Drive integration
â”‚   â”œâ”€â”€ github_reader.py   # GitHub repository processing
â”‚   â”œâ”€â”€ synthetic_reader.py # Synthetic JSON ingestion
â”‚   â”œâ”€â”€ chunker.py         # Text chunking and tagging
â”‚   â”œâ”€â”€ embedder.py        # Embedding generation
â”‚   â”œâ”€â”€ hash_store.py      # Hash store base class
â”‚   â”œâ”€â”€ gdrive_hash_store.py # Google Drive change detection
â”‚   â”œâ”€â”€ synthetic_hash_store.py # Synthetic change detection
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ api/                   # API endpoints
â”‚   â”œâ”€â”€ eval_endpoints.py  # Evaluation API routes
â”‚   â”œâ”€â”€ models.py          # API request/response models
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/                  # Data assets and hash stores (partially gitignored)
â”‚   â”œâ”€â”€ sources/           # Synthetic JSON documents
â”‚   â”œâ”€â”€ writing_samples/   # PDF writing samples
â”‚   â”œâ”€â”€ traits.json
â”‚   â”œâ”€â”€ skills.json
â”‚   â”œâ”€â”€ style.json
â”‚   â”œâ”€â”€ gdrive_name_map.json
â”‚   â”œâ”€â”€ github_hash_store.json
â”‚   â”œâ”€â”€ gdrive_hash_store.json
â”‚   â””â”€â”€ synthetic_hash_store.json
â”‚
â”œâ”€â”€ scripts/               # One-off scripts
â”‚   â””â”€â”€ export_doc_titles.py
â”‚
â”œâ”€â”€ utility/               # Maintenance utilities
â”‚   â”œâ”€â”€ delete_collection.py
â”‚   â”œâ”€â”€ generate_eval.py
â”‚   â””â”€â”€ generate_synthetic_data.py
â”‚
â””â”€â”€ frontend/              # React UI
    â”œâ”€â”€ index.html
    â”œâ”€â”€ package-lock.json
    â”œâ”€â”€ package.json
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ App.jsx        # Main application
    â”‚   â”œâ”€â”€ App.css
    â”‚   â”œâ”€â”€ main.jsx       # Vite entrypoint
    â”‚   â”œâ”€â”€ api/           # API client utilities
    â”‚   â”œâ”€â”€ pages/         # Page components
    â”‚   â”œâ”€â”€ styles/        # Global tokens/styles
    â”‚   â””â”€â”€ mock/          # Local mock data
    â””â”€â”€ vite.config.js
```

---

## ğŸ”‘ Key Features

### Dual Personality Mode
- **Technical**: Responds with technical depth, uses jargon, includes code snippets
- **Non-technical**: Simplified explanations, accessible language, concept-focused

### Content Type Filtering
- Use `@code` prefix to filter retrieval to code-related content only
- Ensures code queries retrieve implementation details, not just documentation

### Smart Retrieval
- Semantic search using OpenAI embeddings
- Configurable similarity thresholds
- Out-of-scope detection for irrelevant queries

### Source Attribution
- All responses include citations to source documents
- Links to original files (Google Drive URLs, GitHub file paths)

### Evaluation Metrics
- **Groundedness**: Checks if response claims are supported by retrieved context
- **Persona Consistency**: Validates alignment with personality mode
- **Retrieval Quality**: Precision, recall, and F1 score tracking

---

## ğŸ› ï¸ API Endpoints

### Core Endpoints

- `POST /api/query` - Submit query and get response
- `GET /api/health` - Health check

### Evaluation Endpoints

- `POST /api/eval/generate-set` - Generate evaluation dataset
- `POST /api/eval/run` - Run evaluation on query set
- `GET /api/eval/results` - Fetch evaluation results

### Ingestion Endpoints

- `POST /api/ingest/gdrive` - Trigger Google Drive ingestion
- `POST /api/ingest/github` - Trigger GitHub ingestion
- `POST /api/ingest/synthetic` - Trigger synthetic data ingestion

---

## ğŸ“ Environment Variables (Optional)

While most configuration is in `config.py`, you can optionally use environment variables:

```bash
export QDRANT_URL="your-qdrant-url"
export QDRANT_API_KEY="your-api-key"
export OPENAI_API_KEY="your-openai-key"
```

## ğŸ“š Configuration Guide

Edit `config.py` to customize:

- **Chunking**: `CHUNK_SIZE`, `CHUNK_OVERLAP`
- **Embedding Model**: `EMBEDDING_MODEL`, `EMBEDDING_DIM`
- **Data Sources**:
  - `TECHNICAL_FOLDER_ID`, `NONTECHNICAL_FOLDER_ID`
  - `GITHUB_REPOS` list
- **Source Types**: `SOURCE_TYPES`, `CONTENT_TYPES`
- **GitHub Settings**: `GITHUB_ALLOWED_EXTENSIONS`, `GITHUB_IGNORE_PATTERNS`

---

## ğŸ§ª Testing

### Test Persona Consistency
```bash
python test_persona_consistency.py
```

### Test Grounding and Bleeding
```bash
python test_bled_full.py
```

### Run Retrieval Evaluation
```bash
python eval_retrieval.py
```

---

--- 

## Steps to setup own ingestion pipeline

### Google Drive
1. Create a Google Cloud project, enable the Google Drive API, and create OAuth client credentials.
2. Download the OAuth client JSON and place it at the repo root as `credentials.json` (same level as `config.py`).
3. Decide which Google Drive folder you want to ingest and copy its folder ID from the URL.
4. Set the folder ID in `config.py` if youâ€™re using `main_ingest.py` (`TECHNICAL_FOLDER_ID` / `NONTECHNICAL_FOLDER_ID`), or pass it directly to `get_gdrive_reader(folder_id)` if you call it yourself
5. Run your ingest flow once; the first run will open a browser for OAuth and create `token.json` at the repo root (this is the cached auth token used on subsequent runs)
6. Re-run ingest as needed; the cached `token.json` is reused automatically

### GitHub
1. Create a GitHub personal access token with read access to the repos you want to ingest
2. Set that token in `config.py` as `GITHUB_TOKEN`
3. Add repo names (`"owner/repo"`) to `GITHUB_REPOS` in `config.py` (or pass a list to `ingest_github(repos=...)`)
4. Verify file filters: `GITHUB_ALLOWED_EXTENSIONS` and `GITHUB_IGNORE_PATTERNS` in `config.py` control what gets ingested
5. Run the ingest flow; it uses `fetch_repo_files()` to traverse repos and pull eligible files

**Built with**: FastAPI, React, Qdrant, OpenAI, LlamaIndex, Google Drive API
