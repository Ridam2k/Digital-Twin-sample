{
  "doc_title": "Multimodal Deep Learning: Speech Emotion Recognition and Fake News Detection",
  "source_url": "",
  "personality_ns": "technical",
  "content_type": "project_writeup",
  "body": "In my work on multimodal deep learning systems, particularly focusing on hierarchical speech emotion recognition and multimodal fake news detection (SEMI-FND), I encountered a variety of challenges and opportunities that exemplify the power and complexity of integrating different modalities\u2014audio, text, and visual features.\n\n### The Challenge of Combining Modalities\n\nOne of the fundamental challenges in multimodal systems lies in effectively combining different data modalities while ensuring that the unique characteristics of each are preserved and leveraged. For instance, audio captures emotional nuances through tone and pitch, text conveys semantic meaning, and visual data can enrich context through facial expressions or gestures. Multimodal approaches often outperform unimodal ones because they provide a more holistic view of the information, allowing for richer feature interactions and more robust predictions. This is particularly evident in tasks like emotion recognition, where emotional expression is often multi-faceted and context-dependent.\n\n### Architecture Design for Fusion\n\nWhen designing architectures for modality fusion, I explored various strategies, including early fusion, late fusion, and hierarchical approaches. \n\n1. **Early Fusion**: This involves concatenating features from different modalities at the input stage. While this can capture direct interactions, it can lead to dimensionality challenges and loss of modality-specific information.\n\n2. **Late Fusion**: This strategy combines predictions from separate unimodal models. While simpler, it often fails to exploit the potential interactions between modalities.\n\n3. **Hierarchical Approaches**: I found hierarchical fusion to be particularly effective. By processing modalities at different levels, I could first capture intra-modal features before integrating them at higher levels. For example, in hierarchical speech emotion recognition, I processed frame-level features to derive utterance-level representations, allowing me to maintain temporal context and emotion dynamics.\n\n### Handling Temporal Hierarchies in Speech Emotion Recognition\n\nIn the context of speech emotion recognition, handling temporal hierarchies was crucial. I utilized convolutional neural networks (CNNs) for frame-level acoustic feature extraction, which allowed me to capture short-term temporal dynamics. Subsequently, I employed recurrent neural networks (RNNs) to aggregate these features into utterance-level representations. This two-tiered structure helped address emotion label ambiguity, as emotional expressions often vary within a single utterance. By training on a dataset with labeled emotional states, I implemented a loss function that accounted for label uncertainty, improving classification robustness.\n\n### Fake News Detection: Integrating Modalities\n\nFor SEMI-FND, I combined text semantics with user engagement patterns and source credibility signals. The text modality was processed using transformer-based models to capture semantic nuances, while engagement metrics (like shares and comments) were analyzed through time-series models to monitor user behavior. Source credibility was encoded as a feature vector based on historical reliability metrics. This multimodal approach enhanced the model's ability to discern subtle cues indicative of misinformation.\n\n### Technical Challenges in Aligning Modalities\n\nAligning modalities presented several technical challenges, such as differing sampling rates and feature dimensionality mismatches. I addressed these by implementing preprocessing steps to standardize input features, including temporal alignment techniques for audio and text modalities. This ensured a coherent representation across all modalities, crucial for effective feature fusion.\n\n### Evaluation Approach and Metrics\n\nMy evaluation approach involved a combination of accuracy, precision, recall, and F1-score to assess the performance of the multimodal systems. Additionally, I utilized ablation studies to evaluate the contribution of each modality. The metrics revealed that while text semantics were pivotal in fake news detection, audio features significantly enhanced emotion recognition accuracy, particularly in emotionally charged contexts.\n\n### Lessons Learned\n\nThrough these experiences, I learned valuable lessons about when multimodal complexity is worth the engineering effort. The added complexity of multimodal systems is justified when:\n\n- The task inherently benefits from multiple perspectives (e.g., emotion recognition).\n- There is sufficient data to train complex models without overfitting.\n- The potential for improved accuracy and robustness outweighs the engineering overhead.\n\nIn conclusion, my work in multimodal deep learning has not only deepened my"
}